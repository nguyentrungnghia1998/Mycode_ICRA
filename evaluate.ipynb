{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/ai-core/miniconda3/envs/contentvec/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM\n",
    "from tqdm import tqdm\n",
    "from pyctcdecode import build_ctcdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inferencer:\n",
    "    def __init__(self, device, huggingface_folder, w2v_model_path, kenlm_model_path, alpha = 1.0):\n",
    "        self.device = device\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(huggingface_folder)\n",
    "        vocab_dict = self.processor.tokenizer.get_vocab()\n",
    "        sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1]) if k not in [\"<s>\", \"</s>\"]}\n",
    "        self.decoder = build_ctcdecoder(\n",
    "            labels=list(sorted_vocab_dict.keys()),\n",
    "            kenlm_model_path=kenlm_model_path,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        # self.processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "        #     feature_extractor=self.processor.feature_extractor,\n",
    "        #     tokenizer=self.processor.tokenizer,\n",
    "        #     decoder=self.decoder\n",
    "        # )\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(huggingface_folder).to(self.device)\n",
    "        if w2v_model_path is not None:\n",
    "            self.preload_model(w2v_model_path)\n",
    "\n",
    "\n",
    "    def preload_model(self, model_path) -> None:\n",
    "        \"\"\"\n",
    "        Preload model parameters (in \"*.tar\" format) at the start of experiment.\n",
    "        Args:\n",
    "            model_path: The file path of the *.tar file\n",
    "        \"\"\"\n",
    "        assert os.path.exists(model_path), f\"The file {model_path} is not exist. please check path.\"\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint, strict = True)\n",
    "        print(f\"Model preloaded successfully from {model_path}.\")\n",
    "\n",
    "\n",
    "    def transcribe(self, wav) -> str:\n",
    "        input_values = self.processor(wav, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "        logits = self.model(input_values.to(self.device)).logits\n",
    "        # Sử dụng KenLM với beam search decoding\n",
    "        pred_transcript = self.decoder.decode(logits.cpu().detach().numpy()[0])  # Chuyển logits sang numpy và giải mã\n",
    "        return pred_transcript\n",
    "\n",
    "    def run(self, test_filepath):\n",
    "        filename = test_filepath.split('/')[-1].split('.')[0]\n",
    "        filetype = test_filepath.split('.')[1]\n",
    "        if filetype == 'txt':\n",
    "            f = open(test_filepath, 'r')\n",
    "            lines = f.read().splitlines()\n",
    "            f.close()\n",
    "\n",
    "            f = open(test_filepath.replace(filename, 'transcript_'+filename), 'w+')\n",
    "            for line in tqdm(lines):\n",
    "                wav, _ = librosa.load(line, sr = 16000)\n",
    "                transcript = self.transcribe(wav)\n",
    "                f.write(line + ' ' + transcript + '\\n')\n",
    "            f.close()\n",
    "\n",
    "        else:\n",
    "            wav, _ = librosa.load(test_filepath, sr = 16000)\n",
    "            print(f\"transcript: {self.transcribe(wav)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = Inferencer(\n",
    "    device = device, \n",
    "    huggingface_folder = \"custom_model\", \n",
    "    w2v_model_path = \"custom_model/pytorch_model.bin\",\n",
    "    kenlm_model_path = \"5gram_correct.arpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
